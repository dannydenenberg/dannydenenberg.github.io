<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  

<!-- for Katex math -->
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
  integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j"
  crossorigin="anonymous"
/>

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
  integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ"
  crossorigin="anonymous"
></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
  crossorigin="anonymous"
  onload="renderMathInElement(document.body);"
></script>
<!-- make the title bigger -->
<style>
  .site-title {
    font-size: 7rem !important;
  }
</style>



  <title>
    
    What the heck is a perceptron? ü•ó &middot; @da_nn_y_o
    
  </title>

  <link rel="stylesheet" href="/styles.css" />
  <link
    rel="apple-touch-icon-precomposed"
    sizes="144x144"
    href="/public/apple-touch-icon-precomposed.png"
  />
  <link rel="shortcut icon" href="/public/favicon.ico" />
  <link
    rel="alternate"
    type="application/atom+xml"
    title="@da_nn_y_o"
    href="/atom.xml"
  />
</head>


  <body>
    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home">@da_nn_y_o</a>
          <small></small>
        </h3>

         &nbsp;&nbsp;&nbsp;<span
          class="nav-bar-item"
          ><a href="/about/">About</a></span
        >
         &nbsp;&nbsp;&nbsp;<span
          class="nav-bar-item"
          ><a href="/dod">Todo</a></span
        >
        
      </header>

      <main>
        <article class="post">
  <h1 class="post-title">What the heck is a perceptron? ü•ó</h1>
  <time datetime="2019-02-24T00:00:00-06:00" class="post-date">February 24, 2019</time>

  <p><em>A quick article about what a perceptron is, how it works, and why it is important.</em></p>
<p>In short, the perceptron is an algorithm. A very simple one at that. <br />The algorithm it performs, in machine learning lingo, is known as <em>binary classification</em>.</p>
<p><strong>Binary classification</strong> is the task of classifying elements into two groups. Hence the word binary‚Ää‚Äî‚Äämeaning 2.</p>
<p>All the algorithm does, it <strong>take an input and produce an output</strong>. The method by which the perceptron produces an output is called the <em>weighted sum</em>. It takes some input X, multiplies it by some weight W, adds in a bias B and then runs that value through some <em>step function</em> which classifies the weighted sum as a 0 or 1 (most of the time).</p>
<blockquote>If there are multiple inputs, then there is a single weight assigned to each input and all of the inputs are multiplied by their corresponding weight and then summed together and added to the bias before being passed through the step function.</blockquote>
<p>Here is visual of what a simple perceptron with <em>n</em> inputs may look like:</p>
<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card"><img src="https://www.denenberg.info/content/images/downloaded_images/What-the-heck-is-a-perceptron-/1-vn7VIPb_JpwEX7PAftjYsw.png" class="kg-image" /></figure>
<!--kg-card-end: image-->
<p>This image depicts the inputs (represented as subscripts of X) each being multiplied by their corresponding weights, summed together, and passed through some step function that takes the weighted sum and produces either a 0 or a 1. Notice that there is a constant that is fed into the perceptron as well as the other inputs. This constant will always be 1 and when multiplied with the first weight, represents the bias. So, essentially, that weight is the bias‚Ää‚Äî‚Ääit is added to the sum of the weights <em>without </em>being multiplied first with an input.</p>
<p>A simple step function is depicted below:</p>
<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://www.denenberg.info/content/images/downloaded_images/What-the-heck-is-a-perceptron-/1-6K2w2TpQmHyphnv5fSvxUg.png" class="kg-image" /><figcaption>Unit Step Function</figcaption></figure>
<!--kg-card-end: image-->
<p>This step function takes some value. If the value is greater than or equal to 0, then it outputs 1, otherwise, it outputs 0.</p>
<h2 id="how-is-it-useful-at-classifying-things">How is it useful at classifying things?</h2>
<p>The perceptron is a subset of machine learning algorithms known as supervised learning. This means that when you teach a perceptron to learn to classify inputs into 2 categories, you give it the inputs and the labels that you want the perceptron to produce in the end (0 or 1 for binary classification, usually).</p>
<p>The perceptron initializes its weights randomly and get‚Äôs fed the inputs. It takes its final output from the step function and compares it to the label that you gave it for the specified set of inputs.</p>
<p>The perceptron can then see whether it was wrong at all and how wrong. It can quantify how wrong it is by producing a loss function which takes the perceptron‚Äôs output and the target label provided by you and produces a number. In essence, the loss function is saying, ‚Äúthis perceptron is 2.76 wrong‚Äù or ‚Äúthis perceptron is 8.92 wrong‚Äù, etc. Here is a common loss function for binary classification known as Squared Error:</p>
<p>$$Loss=(predicted-target)^2$$</p>
<p>So, in a simple example, if the perceptron was fed inputs and produced a 1, but the label assigned to the inputs was 0, then the loss of the perceptron would be:</p>
<p>$$Loss=(1-0)^2=1$$</p>
<p>The perceptron in this case was ‚Äú1 wrong‚Äù.</p>
<blockquote>It can then use this value to adjust the weights is various ways, one of which is known as <strong>Gradient Descent</strong>. In GD, you adjust the weights by subtracting the partial derivative of the loss function with respect to each weight. To actually figure out the derivatives, you would use the chain rule because the loss function is in actuality a composition of the weighted sum plus the bias, piped through the step function:</blockquote>
<p>$$\frac{\partial Loss}{\partial w_i}=\frac{\partial Loss}{\partial y}\frac{\partial y}{\partial w_i}$$</p>
<blockquote>If you are familiar with Calculus and derivatives, this concept shouldn‚Äôt be too difficult to understand and I am planning on writing an article on this soon. But it is beyond the scope of this high-level paper.</blockquote>
<h2 id="what-is-a-binary-classifier">What is a Binary Classifier?</h2>
<p>This means that the input entered is mapped to 2 different categories.</p>
<p>Another way to describe this is by saying that the information entered into a perceptron is linearly separable:</p>
<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://www.denenberg.info/content/images/downloaded_images/What-the-heck-is-a-perceptron-/1-2Y9fKZBzQprHaNGKEEePmw.png" class="kg-image" /><figcaption>Linearly Separable</figcaption></figure>
<!--kg-card-end: image-->
<p>This means that the input entered can be separated by a<strong> single line.</strong></p>
<h2 id="a-perceptron-is-just-like-a-linear-function">A perceptron is just like a linear function</h2>
<p>One thing to note about a perceptron is that it is functions kind of line a linear function. For example, here is a function in slope-intercept form:</p>
<p>$$y=mx+b$$</p>
<p>When comparing this to a perceptron you can see they are much alike. The X is the input, the m is the weight, and the b is the bias. Here is what the perceptron value before it is run through the step function would look like:</p>
<p>$$y=\sum_{i=0}^{n} X_{i} W_{i}+B$$</p>
<p>Looks pretty similar, huh?</p>
<p>This high level overview of a perceptron is a good prep for learning about <strong>Neural Networks</strong> which are just perceptrons stacked on top of each other and gathered into layers.</p>
<p>Just keep learning!

</p>

</article>

<hr />
<div class="qna" style="text-align: center">
  <h2>Questions?</h2>
  <p>
    Have a question about this post, Bootstrap, GitHub, or anything else? Ask
    away on
    <a href="https://twitter.com/" target="_blank"
      >Twitter</a
    >
    or in
    <a href="https://github.com/dannydenenberg/ama" target="_blank"
      >my AMA repo</a
    >.
  </p>
</div>


<!-- 
<aside class="related">
  <h3>Related posts</h3>
  <ul class="related-posts">
    
    <li>
      <a href="/linear-algebra">
        Linear Algebra; The How and Why ü§®
        <small
          ><time datetime="2019-07-04T00:00:00-05:00">04 Jul 2019</time></small
        >
      </a>
    </li>
    
    <li>
      <a href="/ghost-blog-aws-docker">
        How to create a Ghost blog with docker üê≥+ AWS ‚ù§Ô∏è
        <small
          ><time datetime="2019-06-25T00:00:00-05:00">25 Jun 2019</time></small
        >
      </a>
    </li>
    
    <li>
      <a href="/contributing">
        How to contribute on GitHub
        <small
          ><time datetime="2019-04-23T00:00:00-05:00">23 Apr 2019</time></small
        >
      </a>
    </li>
    
  </ul>
</aside>
 -->

      </main>

      <div class="footer" style="text-align: center">
  <hr />
  <p>
    Be notified of new posts.
    <a href="feed://feeds.feedburner.com/mdo">Subscribe to the RSS feed.</a>
  </p>
</div>

    </div>

    
  </body>
</html>
