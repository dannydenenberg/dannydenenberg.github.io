<!DOCTYPE html>
<html lang="en">
  <head>
  <!-- anchors for headings -->
  <script src="../anchor.min.js"></script>

  
    <!-- Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142322478-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-142322478-2');



(async () => {
  // redirect from denenberg.us to dannydenenberg.com
  if (window.location.href.includes('denenberg.us')) {
    let url = window.location.href;
    let firstPart=url.slice(0,8);
    let lastPart=url.slice(20);
    let newUrl = firstPart + 'dannydenenberg.com'+lastPart;
    window.location.replace(newUrl);
  }
})();

</script>
    
    
    <!-- Katex Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  
    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body, {delimiters: [{ left: '$$', right: '$$', display: true },{ left: '\\[', right: '\\]', display: true },{ left: '$', right: '$', display: true },{ left: '\\(', right: '\\)', display: false },{ left: '$$$', right: '$$$', display: true }]});"></script>
<!-- END MATH --> 

    <!-- META INFO -->
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Danny Denenberg's home page on the internet with a programming blog." />

          <!-- END META INFO -->
  <title>
    
    Complete approach to neural networks &middot; Danny Denenberg
    
  </title>

  <!-- Font Awesome Icon Support -->
 <script src="https://kit.fontawesome.com/b0b051f3df.js" crossorigin="anonymous"></script>
  </head>


  <link rel="stylesheet" href="/styles.css" />
  <link
    rel="apple-touch-icon-precomposed"
    sizes="144x144"
    href="/public/apple-touch-icon-precomposed.png"
  />
  <link rel="shortcut icon" href="/public/favicon.ico" />
  <link
    rel="alternate"
    type="application/atom+xml"
    title="Danny Denenberg"
    href="/atom.xml"
  />


</head>


  <body>
    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home">Danny Denenberg</a>
          <small></small>
        </h3>

         &nbsp;&nbsp;&nbsp;<span
          class="nav-bar-item"
          ><a href="/about/">/About</a></span
        >
         &nbsp;&nbsp;&nbsp;<span
          class="nav-bar-item"
          ><a href="/projects">/Projects</a></span
        >
        
      </header>

      <main>
        <article class="post">
  <h1 class="post-title">Complete approach to neural networks</h1>
  <time datetime="2020-01-02T00:00:00-06:00" class="post-date"
    >January 02, 2020</time
  >

  <p><img src="https://cdn-images-1.medium.com/max/2400/1*HbXgRCRHdxsmjRNHEf8q1w.jpeg" alt="" />
<span class="figcaption_hack">Photo by <a href="https://unsplash.com/@seemurray?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Chris
Murray</a>
on
<a href="https://unsplash.com/search/photos/guitar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span></p>

<!-- ### A Math Nerd‚Äôs Complete Approach to Neural Networks

#### Become a neural net rock star! -->

<p>The majority of advances in artificial intelligence in the last half-decade has
been in the developing technology of <strong>artificial neural networks (ANN).</strong></p>

<p>ANNs are created by programming a computer to loosely model the interconnected
brain cells of a human.</p>

<p>An ANN is based on a collection of connected nodes or neurons which represent
singular <a href="https://en.wikipedia.org/wiki/Neuron">neurons</a> in a biological brain.
These nodes are connected through weights which, like
<a href="https://en.wikipedia.org/wiki/Synapse">synapses</a> in the biological brain, can
transmit signals to one another. In an ANN, however, all of these nodes and
weights are simply numbers. The calculations that go into producing a single
value from an entire artificial neural network is <strong>just a series of
multiplication and addition operations.</strong></p>

<p>The field of deep learning further explores the world of ANNs by branching off
and creating recurrent neural networks, generative adversarial networks,
long/short term memory networks, convolution networks, Markov chains, Hopfield
networks, and many many more. However, for the purposes of this article (and
staying sane), I will go in-depth into the intuition behind deep feedforward
neural networks.</p>

<p>Please note if you haven‚Äôt read my article on
<a href="https://medium.com/@dannydenenberg/what-the-heck-is-a-perceptron-62e7311b4073">perceptrons</a>,
please do! (It‚Äôs a fast read and has some info that‚Äôll help with understanding
ANNs)</p>

<blockquote>
  <p>I would also highly suggest reading about <a href="https://medium.com/swlh/univariate-linear-regression-ml-intro-6aba19026186">Univariate Linear
Regression</a>
to gather an understanding of mathematical notation for ANNs and <a href="https://medium.com/@DannyDenenberg/linear-algebra-for-deep-learning-3a4f38a82ba7">Linear
Algebra</a>
to better understand this article. You will need this information. To get a good
understanding of what activation function we use and why, you should read <a href="https://medium.com/@DannyDenenberg/a-practical-comparison-of-activation-functions-6f348be7d274">about
them
here.</a></p>
</blockquote>

<h3 id="its-just-addition">It‚Äôs Just Addition</h3>

<p>Please don‚Äôt do what I did and make ANNs out to be difficult. They simply
aren‚Äôt.</p>

<blockquote>
  <p>Because an ANN is simply layers of <strong>perceptrons</strong>, we will look first at what
that entails.</p>
</blockquote>

<p>For a refresher, a
<a href="https://medium.com/@dannydenenberg/what-the-heck-is-a-perceptron-62e7311b4073">perceptron</a>
has input nodes and a single output node. Each input node is connected by its
own weight synapse to the output node. To calculate the output of a perceptron,
you take the sum of each input node multiplied by its corresponding weight
value.</p>

<p>Let‚Äôs take a look at this simple perceptron:</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*CiTHY-agGfLLH9BwdxsogQ@2x.jpeg" alt="" />
<span class="figcaption_hack">Yeah, I sign my work ü§ì</span></p>

<p>In this case, <em>y</em> is equal to:</p>

<p>$ y=x_1w_1+x_2w_2 $</p>

<p>Or, in a more abstract view:</p>

<p>$y=\sum^m_{i=0}x_iw_i$</p>

<p>where <em>m</em> is the number of inputs and <em>i</em> is the specific input and weight
specified by its index. this sum is known as the ‚Äúweighted sum‚Äù.</p>

<p>An ANN has **layers **of these perceptrons where output layers can have more
than one node. A very simple ANN structure looks like this:</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*i7DQOGziPigzu3iuFOLCQA.jpeg" alt="" />
<span class="figcaption_hack">^3 layer ANN drawn on a Pixel 3 XL :)</span></p>

<p>This is an ANN with 3 layers. As you can see, there is an input and output layer
(column) of nodes, but there is also a 3rd layer in the middle. This is called
the ‚Äúhidden‚Äù layer. The node values in this layer are the outputs from the first
layer and the inputs to the last layer (outputs). If you add a second hidden
layer, the values from the first hidden layer are used in the weighted sum for
the second.</p>

<p>You can add as many hidden layers as you want, although the math gets more
tedious (not harder!) the further you go.</p>

<h3 id="lets-delve">Let‚Äôs Delve</h3>

<p>In the depiction above, each node in a certain layer is connected to every node
in the previous layer through a synapse/weight. This type of network is known as
a **fully connected **network and is the type of ANN that I will be focusing on
in this article.</p>

<p>Because of this connectedness, a node‚Äôs value, as in a perceptron, is the
*weighted sum *of all of the nodes in the previous layer. However, unlike a
perceptron, the weighted sum is not passed through a step function to determine
the final value of an output node. It is passed through some **activation
function **that presents some complex mappings between the weighted sum and the
node‚Äôs actual value. They introduce <em>non-linearity</em> into the network. This
non-linearity is extremely important to the network‚Äôs ability to learn. It is
crucial that the activation function be non-linear or else the entire network
can only conform to linear patterns such as a line of best fit.</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*f3B3KpTAyyS0mL77_rqOnw@2x.png" alt="" />
<span class="figcaption_hack">Boring, linear function</span></p>

<p>With a non-linear activation function, the ANN can conform to complex \(x,y\)
mappings like this one:</p>

<p><br /></p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*PLRDsg8D6MY_KyTcN68vQw@2x.png" alt="" />
<span class="figcaption_hack">Super dope, non-linear function</span></p>

<p>I hope you can now understand some of the key differences between a neural
network and a perceptron. In the next section, I will go into the concept of
<strong>forward propagation</strong>.</p>

<h3 id="producing-outputs">Producing Outputs</h3>

<p><strong>Forward propagation</strong> is the process of taking input values and <em>propagating</em>
them through the network‚Äôs layers until you find the final outputs. Basically,
it is the process of executing the ‚Äòfunction‚Äô of the neural network which is to
take input values and use weights and non-linear activation functions to
transform them into some output values.</p>

<p>For a 3 layer network (depicted
<a href="https://cdn-images-1.medium.com/max/800/1*i7DQOGziPigzu3iuFOLCQA.jpeg">here</a>),
here is the general set of operations for finding the final outputs:</p>

<ol>
  <li>Use first layer input values to find the weighted sum for each of the nodes in
the hidden layer</li>
  <li>Send the hidden layer weighted sums through some <strong>non-linear activation
function</strong></li>
  <li>Use these hidden values to find the weighted sum for each node in the output
layer</li>
  <li>Send the output layer‚Äôs weighted sums through some non-linear activation
function ‚Äî these are the FINAL outputs for the network</li>
</ol>

<p><strong>Let‚Äôs go through all of these steps for our 3 layer network depicted below:</strong></p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*KwphEMrIxTNc-BP7d6LMTQ.png" alt="" />
<span class="figcaption_hack">Clearly, I‚Äôm an MS Paint3D whiz üòé</span></p>

<h4 id="step-1">STEP #1</h4>

<blockquote>
  <p>Use first layer input values to find the weighted sum for each of the nodes in
the hidden layer.</p>
</blockquote>

<p>If we model our inputs (x) and weights (w) as matrices, the math to get to the
second layer becomes much more obvious.</p>

<p><strong>Just a quick note about notation:</strong></p>

<blockquote>
  <p>The weights connecting the input and hidden layers, I will denote as follows:
W·µ¢‚Çï<br /> The weights connecting the hidden and output layers, I will denote as
follows: W‚Çï‚Çí</p>
</blockquote>

<p>$X=\begin{bmatrix}x_1 &amp; x_2\end{bmatrix} \\ \\ W_{ih} = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23}\end{bmatrix}$</p>

<p>Just for a quick reference, here is how you multiply matrices:</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*bcbFuLMZ8ttga7xgyBPCnA.png" alt="" />
<span class="figcaption_hack">Image from
<a href="https://www.mathsisfun.com/algebra/matrix-multiplying.html">mathisfun</a>.com</span></p>

<p>Therefore, to get our <strong>hidden layer weighted sum</strong>, just multiply (take the dot
product of) the input matrix and the weight matrix. The hidden layer nodes are
represented by \(H\).</p>

<p>$H_{weighted-sum}=X \cdot W_{ih}=\begin{bmatrix} (x_1w_{11}+x_2w_{21}) \\ (x_1w_{12}+x_2w_{22}) \\ (x_1w_{13}+x_2w_{23})\end{bmatrix}$</p>

<h4 id="step-2">STEP #2</h4>

<blockquote>
  <p>Send the weighted sum through our activation function.</p>
</blockquote>

<p>For the purposes of simplicity, we will be using the <strong>sigmoid function.</strong> Please read <a href="https://medium.com/@DannyDenenberg/a-practical-comparison-of-activation-functions-6f348be7d274">about it
here</a>
(this article also holds information about other activation functions and their
unique uses in ANNs).</p>

<p>For a quick reference, here is the definition for our sigmoid function:</p>

<p>$\sigma(x)=\frac{1}{1+e^{-x}}$</p>

<p>Now we can pass our weighted sum matrix through this <strong>non-linear</strong> activation function and the outputs of this are our values for our hidden layer nodes.</p>

<p>$H=\sigma(X\times W_{ih})=\begin{bmatrix} \sigma (x_1w_{11}+x_2w_{21}) \\ \sigma(x_1w_{12}+x_2w_{22}) \\ \sigma (x_1w_{13}+x_2w_{23})\end{bmatrix}=\begin{bmatrix} h_1 \\ h_2 \\ h_3 \end{bmatrix}$</p>

<h4 id="step-3">STEP #3</h4>

<blockquote>
  <p>Use these hidden values to find the weighted sum for each node in the output
layer.</p>
</blockquote>

<p>Essentially, we now have to repeat steps 1 and 2 by treating the hidden layer as
the input layer.</p>

<p>We can again model the hidden layer and the weights as their own respective
matrices.</p>

<p>TK hidden layer and weights as matrices math</p>

<p>And then multiply them to find the weighted sum for the output layer.</p>

<p>$Y_{weighted-sum}=H\cdot W_{ho} \\ = \begin{bmatrix} h_1 &amp; h_2 &amp; h_3 \end{bmatrix} \begin{bmatrix} w_{11} &amp; w_{12} \\ w_{21} &amp; w_{22} \\ w_{31} &amp; w_{32}\end{bmatrix} \\ = \begin{bmatrix} h_1w_{11} + h_2w_{21}+h_3w_{31} \\ h_1w_{12}+h_2w_{22}+h_3w_{32}\end{bmatrix}$</p>

<h4 id="step-4-final">STEP #4 (FINAL)</h4>

<blockquote>
  <p>Send the output layer‚Äôs weighted sums through some non-linear activation
function.</p>
</blockquote>

<p>To get the outputs for the entire network, we just have to pass the weighted sum
for the output layer through our activation function (sigmoid once more).</p>

<p>$Y=\sigma (H \cdot W_{ho})=\begin{bmatrix} \sigma (h_1w_{11} + h_2w_{21}+h_3w_{31}) \\ \sigma ( h_1w_{12}+h_2w_{22}+h_3w_{32}) \end{bmatrix}$</p>

<p>And that produces the final ‚Äòoutputs‚Äô for our ANN.</p>

<p>We can call this a network that produces two outputs (one for the first output
node and one for the second). This *concludes *the process of **forward
propagation. **</p>

<h3 id="improving-our-networks-accuracy">Improving our Network‚Äôs Accuracy</h3>

<p>Let‚Äôs say that we created a neural network to act as an <a href="https://en.wikipedia.org/wiki/XOR_gate">XOR
logic</a> evaluator. If it is fed in a 1,0
it should output 1 for true, 0,0 should output 0 for false, etc.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1,0) =&gt; 1
(1,1) =&gt; 0
(0,1) =&gt; 1
(0,0) =&gt; 0
</code></pre></div></div>

<p>If all of the weights are initialized at *random *in our network and we feed it
in a (1,0), there is an infinitesimally small chance that it would actually
output the correct answer of 1. In other words, because the weights are
initialized at random, the ANN ‚Äòfunction‚Äô is not correctly defined.</p>

<p>**What we need is some way to take our randomly initialized network and optimize
it to fit a set of criteria or examples that we provide. **</p>

<p>This method of optimization is known as **backpropagation **and is a central
idea to the structure of neural networks even outside of the 3 layer standard.</p>

<hr />

<p>Before we can begin to optimize our network, we must define a means to tell just
_how wrong of outputs the network produced. _</p>

<p>From the example above, if I fed in (1,0) and the network gave out 0, I need to
define just how ‚Äúwrong‚Äù that specific output was.</p>

<p>This is known as defining an <strong>error function</strong>. Although there are many,
complex and interesting definitions, for the sake of taking easy derivatives, I
will be using a simplistic version of what is known as **squared error. **</p>

<blockquote>
  <p>E = (≈∑ ‚Äî y)¬≤</p>
</blockquote>

<p><em>≈∑</em> is the output that the ANN **produced **and *y *is the **expected **output
of the network (a.k.a. the ‚Äòcorrect‚Äô answer).</p>

<p>So, in the example explained above where (1,0) was inputted and the ANN
outputted 0 (the wrong answer), the error would be:</p>

<blockquote>
  <p>E = (0‚Äì1)¬≤ <br /> = 1</p>
</blockquote>

<p>Now that we have this idea of an error function, we can begin to actually
optimize our network‚Äôs performance through an algorithm known as <strong>gradient
descent.</strong></p>

<hr />

<p>If we take a look at the graph of our error function for a data sample, like the
(1,0) above, it would look something like this:</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*vAf00SS5A7Mn-NQQTdzOWg.png" alt="" />
<span class="figcaption_hack">Graph of Error Function:** y = (x ‚Äî 1)¬≤**</span></p>

<p>As you can see, the further the outputs from the network (inputs to the error
function above) stray from the correct answer, 1, the error grows.</p>

<p>We can also clearly see that the optimum value for the network occurs when the
output is 1 because this is the value at which the error is the smallest (0
error). This optimal value happens at the** vertex** of the function ‚Äî where
the slope is zero.</p>

<p>Therefore, because the derivative of a function describes its instantaneous rate
of change at any point, the optimal value can also be described as the point at
which the <strong>error function‚Äôs derivative is 0</strong>.</p>

<p>$\frac{dE}{dx}=0$</p>

<p>This can also be rewritten as</p>

<p>$\frac{dE}{d≈∑}=0$</p>

<p>This process of finding where the slope of a function is zero in order to find a
minimum (or maximum, if the vertex is a high point) is known as <strong>minimizing a
function.</strong> It‚Äôs a super common problem in calculus and the real world.</p>

<hr />

<p>In the function above, it was immediately clear what the optimal value of x was.</p>

<p>In ANNs, however, the error produced is based on large numbers of weights. Each
individual weight is an independent variable that affects the error function in
its own manner. Thus, using each weight as inputs to the error function, the
graph suddenly becomes hugely complex and exists in many dimensions. Below is an
example of a simplified error function graphed in 3 dimensions:</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*xMhipwY3CNVsQ5WLTsbpSg.png" alt="" />
<span class="figcaption_hack">3D Error Function</span></p>

<p>As you can see, simply by adding a single dimension, it becomes much more
difficult to pinpoint the optimal value by looking at the graph.</p>

<p><strong>TODO:</strong> the output that produces the lowest error occurs at the lowest point of the
graph. that point is where the derivative is zero (horizontal line). Explain how
you can start from anywhere and subtract the derivative and get to the min. Also
explain WHY we use this method (b/c there are much more complex error functions
that produce 3d, 4d, 5d, etc. graphs ‚Äî show picture).</p>

<p><strong>Note</strong>: This article is not yet done. I put it on my site simply to allow people to learn before the article is written in its entirety.</p>

</article>

<hr />

<div class="qna">
  <h2>Questions?</h2>
  <p>
    Have a question about this post, the right way to tell a theater joke üé≠, or
    anything else? Ask away on
    <a href="https://github.com/dannydenenberg/ama" target="_blank"
      >my AMA repo</a
    >
    or shoot me an <a href="/about" target="_blank">email</a>.
  </p>
</div>


<!-- 
<aside class="related">
  <h3>Related posts</h3>
  <ul class="related-posts">
    
    <li>
      <a href="/jwt-express">
        JSON Web Tokens with ExpressJS
        <small
          ><time datetime="2020-02-14T00:00:00-06:00">14 Feb 2020</time></small
        >
      </a>
    </li>
    
    <li>
      <a href="/activation-functions-comparison">
        Activation functions comparison
        <small
          ><time datetime="2020-02-12T00:00:00-06:00">12 Feb 2020</time></small
        >
      </a>
    </li>
    
    <li>
      <a href="/gradient-descent-univar-lr">
        Gradient descent for univariate linear regression
        <small
          ><time datetime="2019-09-20T00:00:00-05:00">20 Sep 2019</time></small
        >
      </a>
    </li>
    
  </ul>
</aside>
 -->

      </main>

      <div class="footer" style="text-align: center">
  <a
    style="text-decoration: none"
    href="https://github.com/dannydenenberg/dannydenenberg.github.io"
    ><3</a
  >
</div>

    </div>

    

    <!-- PUt the anchors on headings -->
    <script>
      // https://github.com/bryanbraun/anchorjs
      anchors.options = {
        placement: "left",
        // visible: "always"
        icon: "¬∂"
      };
      anchors.add("main h2, main h3, main h4, main h5");
    </script>
  </body>
</html>
