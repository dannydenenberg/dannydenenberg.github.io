<!DOCTYPE html>
<html lang="en">
  <head>
  <!-- anchors for headings -->
  <script src="../anchor.min.js"></script>

  
    <!-- Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142322478-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-142322478-2');
</script>
    
    
    <!-- Katex Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  
    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body, {delimiters: [{ left: '$$', right: '$$', display: true },{ left: '\\[', right: '\\]', display: true },{ left: '$', right: '$', display: true },{ left: '\\(', right: '\\)', display: false },{ left: '$$$', right: '$$$', display: true }]});"></script>
<!-- END MATH --> 

    <!-- META INFO -->
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Danny Denenberg's home page on the internet with a programming blog." />

          <!-- END META INFO -->
  <title>
    
    Activation functions comparison &middot; Danny Denenberg
    
  </title>

  <!-- Font Awesome Icon Support -->
 <script src="https://kit.fontawesome.com/b0b051f3df.js" crossorigin="anonymous"></script>
  </head>


  <link rel="stylesheet" href="/styles.css" />
  <link
    rel="apple-touch-icon-precomposed"
    sizes="144x144"
    href="/public/apple-touch-icon-precomposed.png"
  />
  <link rel="shortcut icon" href="/public/favicon.ico" />
  <link
    rel="alternate"
    type="application/atom+xml"
    title="Danny Denenberg"
    href="/atom.xml"
  />


</head>


  <body>
    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home">Danny Denenberg</a>
          <small></small>
        </h3>

         &nbsp;&nbsp;&nbsp;<span
          class="nav-bar-item"
          ><a href="/about/">About</a></span
        >
         &nbsp;&nbsp;&nbsp;<span
          class="nav-bar-item"
          ><a href="/projects">Projects</a></span
        >
        
      </header>

      <main>
        <article class="post">
  <h1 class="post-title">Activation functions comparison</h1>
  <time datetime="2020-02-12T00:00:00-06:00" class="post-date"
    >February 12, 2020</time
  >

  <style>
  .post img, .post iframe {;
    background-color: white;
}
</style>

<p>Consider a neuron. All it does is calculate the sum of its ‚Äúweighted‚Äù inputs and adds the bias on that:</p>

<p>$y=\sum (weight \times input) + bias$</p>

<p>Because Y can be any value from (-inf, +inf), activation functions are the deciding factor of whether a neuron should fire or not (be on or off, 0 or 1, etc.)</p>

<p><strong>Why use an activation function?</strong> Well, there are a few good reasons, the most important being that activation functions introduce non-linearity into the network. A neural network without activation functions is basically just a linear regression model and is not able to do more complicated tasks such as language translations and image classifications. Also, linear functions are not able to make use of backpropagation which is the way neural networks ‚Äúlearn‚Äù.</p>

<h2 id="step-function">Step Function</h2>

<p>This is a function that says:</p>

<blockquote>
  <p>\(Activated=true\) if Y &gt; some number (usually 0), otherwise 0</p>
</blockquote>

<p><img src="/goods/step-function.png" alt="Step Function" /></p>

<p>What happens when there are many different neurons that are all 1 or all 0 or some are 1 and some are 0. How do you decide which is most right? This is what activation functions help with.</p>

<p>What if I had some function that could tell me which is most right‚Ä¶.20% right 99% right 87%, etc‚Ä¶.</p>

<p>The first thing that comes to mind is a Linear Function:</p>

<p>$A=cx$</p>

<p>This way it gives a range of activations and given many neurons we could choose a max or min or something else.</p>

<p>The problem with this is that the derivative of this linear function is a constant which means that in the back-propagation of the network, the derivative will never go to zero and find that ‚Äúminimum‚Äù. It will keep climbing slowly to either +inf or -inf.</p>

<p>Let‚Äôs now look at a function that can give us a range of results but is non-linear.</p>

<h2 id="sigmoid">Sigmoid</h2>

<p>$A=\frac{1}{1+e^{-x}}$</p>

<p><img src="/goods/sigmoid.png" alt="" /></p>

<p><strong>Benefits of this ‚Äòstep‚Äô function:</strong></p>

<ul>
  <li>Non-linear</li>
  <li>Binary (ever output is either above, below, or equal to 0.5)</li>
  <li>Smooth gradient</li>
  <li>Always will be in the range (0,1) so values are kept close together</li>
</ul>

<p>A downside of the sigmoid function is that towards the ends of the curve, y values (outputs) tend to respond very little to changes in x (\(\frac{dy}{dx}\) is close to zero). This means that the gradient is small or has vanished entirely which gives useless results for inputs that are higher than about 6 in value.</p>

<h2 id="tanh">Tanh</h2>

<p>$tanh(x)=\frac{2}{1+e^{-2x}}-1$</p>

<p><img src="/goods/tanh.png" alt="" /></p>

<p>As you may be able to see, it is a scaled sigmoid function:</p>

<p>$tanh(x)=2sigmoid(2x)-1$</p>

<p><strong>About the Tanh function:</strong></p>

<ul>
  <li>Non-linear</li>
  <li>Is in range (-1,1)</li>
  <li>The gradient is stronger for tanh that sigmoid (the derivatives are much steeper more of the time)</li>
  <li>Like sigmoid, tanh also has the vanishing gradient problem towards the ends</li>
  <li>Like sigmoid, it is very popular ‚Äî libraries support it</li>
</ul>

<h2 id="relu-rectified-linear-unit">ReLu (rectified linear unit)</h2>

<p>$A(x)=max(0,x)$</p>

<p><img src="/goods/relu.png" alt="" /></p>

<p><strong>About ReLu:</strong></p>

<ul>
  <li>Non-linear in nature.</li>
  <li>Not bound (darn!)</li>
  <li>Range is [0, inf) ‚Äî can blow up the activation</li>
  <li>Not as computationally costly as the other functions</li>
  <li>Involves only simple mathematical operations</li>
  <li>Because of the horizontal line for any negative x values, the gradient can go to 0 and nothing changes in the back-propagation step‚Äî this is known as the ‚Äúdying ReLu problem‚Äù</li>
</ul>

<h2 id="which-one">Which one??</h2>

<p>Sigmoid has proven to work well on <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification problems</a>, however, both sigmoid and tanh suffer from the ‚Äúvanishing gradient‚Äù issue. They are generally not recommended nowadays because of loss in performance and accuracy in deep neural networks.</p>

<p>The ReLu activation function is widely used and is the default choice for many as it yields better results although it should only be used in hidden layers because the model can suffer from dead neurons or infinite max out otherwise. If you have a <a href="https://en.wikipedia.org/wiki/Linear_regression">regression problem</a>, then the final layer of your network could possibly use a linear function (though not recommended a ton when using it in conjunction with an ANN).</p>

<p>I hope this article helped clear out some misconceptions or confusion around the different activation functions!</p>

</article>

<hr />

<div class="qna">
  <h2>Questions?</h2>
  <p>
    Have a question about this post, the right way to tell a theater joke üé≠, or
    anything else? Ask away on
    <a href="https://github.com/dannydenenberg/ama" target="_blank"
      >my AMA repo</a
    >
    or shoot me an <a href="/about" target="_blank">email</a>.
  </p>
</div>


<!-- 
<aside class="related">
  <h3>Related posts</h3>
  <ul class="related-posts">
    
    <li>
      <a href="/jwt-express">
        JSON Web Tokens with ExpressJS
        <small
          ><time datetime="2020-02-14T00:00:00-06:00">14 Feb 2020</time></small
        >
      </a>
    </li>
    
    <li>
      <a href="/complete-neural-nets">
        Complete approach to neural networks
        <small
          ><time datetime="2020-01-02T00:00:00-06:00">02 Jan 2020</time></small
        >
      </a>
    </li>
    
    <li>
      <a href="/gradient-descent-univar-lr">
        Gradient descent for univariate linear regression
        <small
          ><time datetime="2019-09-20T00:00:00-05:00">20 Sep 2019</time></small
        >
      </a>
    </li>
    
  </ul>
</aside>
 -->

      </main>

      <div class="footer" style="text-align: center">
  <a
    style="text-decoration: none"
    href="https://github.com/dannydenenberg/dannydenenberg.github.io"
    ><3</a
  >
</div>

    </div>

    

    <!-- PUt the anchors on headings -->
    <script>
      // https://github.com/bryanbraun/anchorjs
      anchors.options = {
        placement: "left",
        // visible: "always"
        icon: "¬∂"
      };
      anchors.add("main h2, main h3, main h4, main h5");
    </script>
  </body>
</html>
